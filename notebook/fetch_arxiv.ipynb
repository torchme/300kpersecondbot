{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "\n",
    "def get_citations(author_name):\n",
    "    search_query = scholarly.search_author(author_name)\n",
    "    try:\n",
    "        author = next(search_query)\n",
    "        author = scholarly.fill(author, sections=[\"basics\", \"indices\"])\n",
    "\n",
    "        h_index = author.get(\"hindex\", 0)\n",
    "        i10_index = author.get(\"i10index\", 0)\n",
    "        return h_index, i10_index\n",
    "    except StopIteration:\n",
    "        print(\"Автор не найден\")\n",
    "        return 0, 0\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка: {e}\")\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fetch_arxiv_papers(query, max_results=10):\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    query = f'search_query=all:\"{query}\"&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
    "    url = base_url + query\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Ошибка при запросе: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    root = ET.fromstring(response.content)\n",
    "    papers = []\n",
    "    for entry in tqdm(root.findall(\"{http://www.w3.org/2005/Atom}entry\")):\n",
    "        authors = [author.find(\"{http://www.w3.org/2005/Atom}name\").text for author in entry.findall(\"{http://www.w3.org/2005/Atom}author\")]\n",
    "        paper_info = {\n",
    "            \"title\": entry.find(\"{http://www.w3.org/2005/Atom}title\").text,\n",
    "            \"summary\": entry.find(\"{http://www.w3.org/2005/Atom}summary\").text,\n",
    "            \"published\": entry.find(\"{http://www.w3.org/2005/Atom}published\").text,\n",
    "            \"link\": entry.find(\"{http://www.w3.org/2005/Atom}id\").text.strip(),\n",
    "            \"authors\": authors\n",
    "        }\n",
    "\n",
    "        h_indices = []\n",
    "        i10_indices = []\n",
    "\n",
    "        for author in authors:\n",
    "            h_index, i10_index = get_citations(author)\n",
    "            h_indices.append(h_index)\n",
    "            i10_indices.append(i10_index)\n",
    "\n",
    "        average_h_index = sum(h_indices) / len(h_indices) if h_indices else 0\n",
    "        average_i10_index = sum(i10_indices) / len(i10_indices) if i10_indices else 0\n",
    "\n",
    "        paper_info[\"h_index\"] = average_h_index\n",
    "        paper_info[\"i10_index\"] = average_i10_index\n",
    "\n",
    "        papers.append(paper_info)\n",
    "\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:42<04:18, 15.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [01:04<04:41, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:07<03:07, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:22<03:06, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [02:10<05:20, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [02:27<04:26, 22.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [02:47<03:56, 21.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [03:24<03:00, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n",
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [03:59<02:12, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n",
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [05:40<01:53, 28.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [07:37<01:29, 44.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [07:46<00:33, 33.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:46<00:00, 26.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Автор не найден\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "papers_data = fetch_arxiv_papers(\"recommender systems\", max_results=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Can Large Language Models Assess Serendipity in Recommender Systems?',\n",
       "  'summary': \"  Serendipity-oriented recommender systems aim to counteract\\nover-specialization in user preferences. However, evaluating a user's\\nserendipitous response towards a recommended item can be challenging because of\\nits emotional nature. In this study, we address this issue by leveraging the\\nrich knowledge of large language models (LLMs), which can perform a variety of\\ntasks. First, this study explored the alignment between serendipitous\\nevaluations made by LLMs and those made by humans. In this investigation, a\\nbinary classification task was given to the LLMs to predict whether a user\\nwould find the recommended item serendipitously. The predictive performances of\\nthree LLMs on a benchmark dataset in which humans assigned the ground truth of\\nserendipitous items were measured. The experimental findings reveal that\\nLLM-based assessment methods did not have a very high agreement rate with human\\nassessments. However, they performed as well as or better than the baseline\\nmethods. Further validation results indicate that the number of user rating\\nhistories provided to LLM prompts should be carefully chosen to avoid both\\ninsufficient and excessive inputs and that the output of LLMs that show high\\nclassification performance is difficult to interpret.\\n\",\n",
       "  'published': '2024-04-11T06:22:56Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.07499v1',\n",
       "  'authors': ['Yu Tokutake', 'Kazushi Okamoto'],\n",
       "  'h_index': 2.0,\n",
       "  'i10_index': 0.0},\n",
       " {'title': 'NFARec: A Negative Feedback-Aware Recommender Model',\n",
       "  'summary': \"  Graph neural network (GNN)-based models have been extensively studied for\\nrecommendations, as they can extract high-order collaborative signals\\naccurately which is required for high-quality recommender systems. However,\\nthey neglect the valuable information gained through negative feedback in two\\naspects: (1) different users might hold opposite feedback on the same item,\\nwhich hampers optimal information propagation in GNNs, and (2) even when an\\nitem vastly deviates from users' preferences, they might still choose it and\\nprovide a negative rating. In this paper, we propose a negative feedback-aware\\nrecommender model (NFARec) that maximizes the leverage of negative feedback. To\\ntransfer information to multi-hop neighbors along an optimal path effectively,\\nNFARec adopts a feedback-aware correlation that guides hypergraph convolutions\\n(HGCs) to learn users' structural representations. Moreover, NFARec\\nincorporates an auxiliary task - predicting the feedback sentiment polarity\\n(i.e., positive or negative) of the next interaction - based on the Transformer\\nHawkes Process. The task is beneficial for understanding users by learning the\\nsentiment expressed in their previous sequential feedback patterns and\\npredicting future interactions. Extensive experiments demonstrate that NFARec\\noutperforms competitive baselines. Our source code and data are released at\\nhttps://github.com/WangXFng/NFARec.\\n\",\n",
       "  'published': '2024-04-10T10:45:30Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.06900v1',\n",
       "  'authors': ['Xinfeng Wang',\n",
       "   'Fumiyo Fukumoto',\n",
       "   'Jin Cui',\n",
       "   'Yoshimi Suzuki',\n",
       "   'Dongjin Yu'],\n",
       "  'h_index': 31.8,\n",
       "  'i10_index': 69.0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Can Large Language Models Assess Serendipity in Recommender Systems?',\n",
       "  'summary': \"  Serendipity-oriented recommender systems aim to counteract\\nover-specialization in user preferences. However, evaluating a user's\\nserendipitous response towards a recommended item can be challenging because of\\nits emotional nature. In this study, we address this issue by leveraging the\\nrich knowledge of large language models (LLMs), which can perform a variety of\\ntasks. First, this study explored the alignment between serendipitous\\nevaluations made by LLMs and those made by humans. In this investigation, a\\nbinary classification task was given to the LLMs to predict whether a user\\nwould find the recommended item serendipitously. The predictive performances of\\nthree LLMs on a benchmark dataset in which humans assigned the ground truth of\\nserendipitous items were measured. The experimental findings reveal that\\nLLM-based assessment methods did not have a very high agreement rate with human\\nassessments. However, they performed as well as or better than the baseline\\nmethods. Further validation results indicate that the number of user rating\\nhistories provided to LLM prompts should be carefully chosen to avoid both\\ninsufficient and excessive inputs and that the output of LLMs that show high\\nclassification performance is difficult to interpret.\\n\",\n",
       "  'published': '2024-04-11T06:22:56Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.07499v1',\n",
       "  'authors': ['Yu Tokutake', 'Kazushi Okamoto'],\n",
       "  'h_index': 2.0,\n",
       "  'i10_index': 0.0},\n",
       " {'title': 'NFARec: A Negative Feedback-Aware Recommender Model',\n",
       "  'summary': \"  Graph neural network (GNN)-based models have been extensively studied for\\nrecommendations, as they can extract high-order collaborative signals\\naccurately which is required for high-quality recommender systems. However,\\nthey neglect the valuable information gained through negative feedback in two\\naspects: (1) different users might hold opposite feedback on the same item,\\nwhich hampers optimal information propagation in GNNs, and (2) even when an\\nitem vastly deviates from users' preferences, they might still choose it and\\nprovide a negative rating. In this paper, we propose a negative feedback-aware\\nrecommender model (NFARec) that maximizes the leverage of negative feedback. To\\ntransfer information to multi-hop neighbors along an optimal path effectively,\\nNFARec adopts a feedback-aware correlation that guides hypergraph convolutions\\n(HGCs) to learn users' structural representations. Moreover, NFARec\\nincorporates an auxiliary task - predicting the feedback sentiment polarity\\n(i.e., positive or negative) of the next interaction - based on the Transformer\\nHawkes Process. The task is beneficial for understanding users by learning the\\nsentiment expressed in their previous sequential feedback patterns and\\npredicting future interactions. Extensive experiments demonstrate that NFARec\\noutperforms competitive baselines. Our source code and data are released at\\nhttps://github.com/WangXFng/NFARec.\\n\",\n",
       "  'published': '2024-04-10T10:45:30Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.06900v1',\n",
       "  'authors': ['Xinfeng Wang',\n",
       "   'Fumiyo Fukumoto',\n",
       "   'Jin Cui',\n",
       "   'Yoshimi Suzuki',\n",
       "   'Dongjin Yu'],\n",
       "  'h_index': 31.8,\n",
       "  'i10_index': 69.0},\n",
       " {'title': \"Echo Chambers in the Age of Algorithms: An Audit of Twitter's Friend\\n  Recommender System\",\n",
       "  'summary': \"  The presence of political misinformation and ideological echo chambers on\\nsocial media platforms is concerning given the important role that these sites\\nplay in the public's exposure to news and current events. Algorithmic systems\\nemployed on these platforms are presumed to play a role in these phenomena, but\\nlittle is known about their mechanisms and effects. In this work, we conduct an\\nalgorithmic audit of Twitter's Who-To-Follow friend recommendation system, the\\nfirst empirical audit that investigates the impact of this algorithm in-situ.\\nWe create automated Twitter accounts that initially follow left and right\\naffiliated U.S. politicians during the 2022 U.S. midterm elections and then\\ngrow their information networks using the platform's recommender system. We\\npair the experiment with an observational study of Twitter users who already\\nfollow the same politicians. Broadly, we find that while following the\\nrecommendation algorithm leads accounts into dense and reciprocal neighborhoods\\nthat structurally resemble echo chambers, the recommender also results in less\\npolitical homogeneity of a user's network compared to accounts growing their\\nnetworks through social endorsement. Furthermore, accounts that exclusively\\nfollowed users recommended by the algorithm had fewer opportunities to\\nencounter content centered on false or misleading election narratives compared\\nto choosing friends based on social endorsement.\\n\",\n",
       "  'published': '2024-04-09T16:12:22Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.06422v1',\n",
       "  'authors': ['Kayla Duskin',\n",
       "   'Joseph S. Schafer',\n",
       "   'Jevin D. West',\n",
       "   'Emma S. Spiro'],\n",
       "  'h_index': 26.0,\n",
       "  'i10_index': 45.5},\n",
       " {'title': 'DRE: Generating Recommendation Explanations by Aligning Large Language\\n  Models at Data-level',\n",
       "  'summary': '  Recommendation systems play a crucial role in various domains, suggesting\\nitems based on user behavior.However, the lack of transparency in presenting\\nrecommendations can lead to user confusion. In this paper, we introduce\\nData-level Recommendation Explanation (DRE), a non-intrusive explanation\\nframework for black-box recommendation models.Different from existing methods,\\nDRE does not require any intermediary representations of the recommendation\\nmodel or latent alignment training, mitigating potential performance issues.We\\npropose a data-level alignment method, leveraging large language models to\\nreason relationships between user data and recommended items.Additionally, we\\naddress the challenge of enriching the details of the explanation by\\nintroducing target-aware user preference distillation, utilizing item reviews.\\nExperimental results on benchmark datasets demonstrate the effectiveness of the\\nDRE in providing accurate and user-centric explanations, enhancing user\\nengagement with recommended item.\\n',\n",
       "  'published': '2024-04-09T13:44:16Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.06311v1',\n",
       "  'authors': ['Shen Gao',\n",
       "   'Yifan Wang',\n",
       "   'Jiabao Fang',\n",
       "   'Lisi Chen',\n",
       "   'Peng Han',\n",
       "   'Shuo Shang'],\n",
       "  'h_index': 19.666666666666668,\n",
       "  'i10_index': 45.333333333333336},\n",
       " {'title': 'Dimensionality Reduction in Sentence Transformer Vector Databases with\\n  Fast Fourier Transform',\n",
       "  'summary': '  Dimensionality reduction in vector databases is pivotal for streamlining AI\\ndata management, enabling efficient storage, faster computation, and improved\\nmodel performance. This paper explores the benefits of reducing vector database\\ndimensions, with a focus on computational efficiency and overcoming the curse\\nof dimensionality. We introduce a novel application of Fast Fourier Transform\\n(FFT) to dimensionality reduction, a method previously underexploited in this\\ncontext. By demonstrating its utility across various AI domains, including\\nRetrieval-Augmented Generation (RAG) models and image processing, this\\nFFT-based approach promises to improve data retrieval processes and enhance the\\nefficiency and scalability of AI solutions. The incorporation of FFT may not\\nonly optimize operations in real-time processing and recommendation systems but\\nalso extend to advanced image processing techniques, where dimensionality\\nreduction can significantly improve performance and analysis efficiency. This\\npaper advocates for the broader adoption of FFT in vector database management,\\nmarking a significant stride towards addressing the challenges of data volume\\nand complexity in AI research and applications. Unlike many existing\\napproaches, we directly handle the embedding vectors produced by the model\\nafter processing a test input.\\n',\n",
       "  'published': '2024-04-09T13:02:22Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.06278v1',\n",
       "  'authors': ['Vitaly Bulgakov', 'Alec Segal'],\n",
       "  'h_index': 0.0,\n",
       "  'i10_index': 0.0},\n",
       " {'title': 'Exploring Diverse Sounds: Identifying Outliers in a Music Corpus',\n",
       "  'summary': \"  Existing research on music recommendation systems primarily focuses on\\nrecommending similar music, thereby often neglecting diverse and distinctive\\nmusical recordings. Musical outliers can provide valuable insights due to the\\ninherent diversity of music itself. In this paper, we explore music outliers,\\ninvestigating their potential usefulness for music discovery and recommendation\\nsystems. We argue that not all outliers should be treated as noise, as they can\\noffer interesting perspectives and contribute to a richer understanding of an\\nartist's work. We introduce the concept of 'Genuine' music outliers and provide\\na definition for them. These genuine outliers can reveal unique aspects of an\\nartist's repertoire and hold the potential to enhance music discovery by\\nexposing listeners to novel and diverse musical experiences.\\n\",\n",
       "  'published': '2024-04-09T08:10:04Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.06103v1',\n",
       "  'authors': ['Le Cai', 'Sam Ferguson', 'Gengfa Fang', 'Hani Alshamrani'],\n",
       "  'h_index': 24.75,\n",
       "  'i10_index': 40.75},\n",
       " {'title': 'End-to-end training of Multimodal Model and ranking Model',\n",
       "  'summary': '  Traditional recommender systems heavily rely on ID features, which often\\nencounter challenges related to cold-start and generalization. Modeling\\npre-extracted content features can mitigate these issues, but is still a\\nsuboptimal solution due to the discrepancies between training tasks and model\\nparameters. End-to-end training presents a promising solution for these\\nproblems, yet most of the existing works mainly focus on retrieval models,\\nleaving the multimodal techniques under-utilized. In this paper, we propose an\\nindustrial multimodal recommendation framework named EM3: End-to-end training\\nof Multimodal Model and ranking Model, which sufficiently utilizes multimodal\\ninformation and allows personalized ranking tasks to directly train the core\\nmodules in the multimodal model to obtain more task-oriented content features,\\nwithout overburdening resource consumption. First, we propose Fusion-Q-Former,\\nwhich consists of transformers and a set of trainable queries, to fuse\\ndifferent modalities and generate fixed-length and robust multimodal\\nembeddings. Second, in our sequential modeling for user content interest, we\\nutilize Low-Rank Adaptation technique to alleviate the conflict between huge\\nresource consumption and long sequence length. Third, we propose a novel\\nContent-ID-Contrastive learning task to complement the advantages of content\\nand ID by aligning them with each other, obtaining more task-oriented content\\nembeddings and more generalized ID embeddings. In experiments, we implement EM3\\non different ranking models in two scenario, achieving significant improvements\\nin both offline evaluation and online A/B test, verifying the generalizability\\nof our method. Ablation studies and visualization are also performed.\\nFurthermore, we also conduct experiments on two public datasets to show that\\nour proposed method outperforms the state-of-the-art methods.\\n',\n",
       "  'published': '2024-04-09T07:32:55Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.06078v1',\n",
       "  'authors': ['Xiuqi Deng',\n",
       "   'Lu Xu',\n",
       "   'Xiyao Li',\n",
       "   'Jinkai Yu',\n",
       "   'Erpeng Xue',\n",
       "   'Zhongyuan Wang',\n",
       "   'Di Zhang',\n",
       "   'Zhaojie Liu',\n",
       "   'Guorui Zhou',\n",
       "   'Yang Song',\n",
       "   'Na Mou',\n",
       "   'Shen Jiang',\n",
       "   'Han Li'],\n",
       "  'h_index': 37.92307692307692,\n",
       "  'i10_index': 180.15384615384616},\n",
       " {'title': 'Wasserstein Dependent Graph Attention Network for Collaborative\\n  Filtering with Uncertainty',\n",
       "  'summary': '  Collaborative filtering (CF) is an essential technique in recommender systems\\nthat provides personalized recommendations by only leveraging user-item\\ninteractions. However, most CF methods represent users and items as fixed\\npoints in the latent space, lacking the ability to capture uncertainty. In this\\npaper, we propose a novel approach, called the Wasserstein dependent Graph\\nATtention network (W-GAT), for collaborative filtering with uncertainty. We\\nutilize graph attention network and Wasserstein distance to address the\\nlimitations of LightGCN and Kullback-Leibler divergence (KL) divergence to\\nlearn Gaussian embedding for each user and item. Additionally, our method\\nincorporates Wasserstein-dependent mutual information further to increase the\\nsimilarity between positive pairs and to tackle the challenges induced by KL\\ndivergence. Experimental results on three benchmark datasets show the\\nsuperiority of W-GAT compared to several representative baselines. Extensive\\nexperimental analysis validates the effectiveness of W-GAT in capturing\\nuncertainty by modeling the range of user preferences and categories associated\\nwith items.\\n',\n",
       "  'published': '2024-04-09T02:51:09Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.05962v1',\n",
       "  'authors': ['Haoxuan Li',\n",
       "   'Yuanxin Ouyang',\n",
       "   'Zhuang Liu',\n",
       "   'Wenge Rong',\n",
       "   'Zhang Xiong'],\n",
       "  'h_index': 55.6,\n",
       "  'i10_index': 142.0},\n",
       " {'title': 'Balancing Information Perception with Yin-Yang: Agent-Based Information\\n  Neutrality Model for Recommendation Systems',\n",
       "  'summary': \"  While preference-based recommendation algorithms effectively enhance user\\nengagement by recommending personalized content, they often result in the\\ncreation of ``filter bubbles''. These bubbles restrict the range of information\\nusers interact with, inadvertently reinforcing their existing viewpoints.\\nPrevious research has focused on modifying these underlying algorithms to\\ntackle this issue. Yet, approaches that maintain the integrity of the original\\nalgorithms remain largely unexplored. This paper introduces an Agent-based\\nInformation Neutrality model grounded in the Yin-Yang theory, namely, AbIN.\\nThis innovative approach targets the imbalance in information perception within\\nexisting recommendation systems. It is designed to integrate with these\\npreference-based systems, ensuring the delivery of recommendations with neutral\\ninformation. Our empirical evaluation of this model proved its efficacy,\\nshowcasing its capacity to expand information diversity while respecting user\\npreferences. Consequently, AbIN emerges as an instrumental tool in mitigating\\nthe negative impact of filter bubbles on information consumption.\\n\",\n",
       "  'published': '2024-04-07T10:16:22Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.04906v1',\n",
       "  'authors': ['Mengyan Wang',\n",
       "   'Yuxuan Hu',\n",
       "   'Shiqing Wu',\n",
       "   'Weihua Li',\n",
       "   'Quan Bai',\n",
       "   'Verica Rupar'],\n",
       "  'h_index': 21.833333333333332,\n",
       "  'i10_index': 75.5},\n",
       " {'title': 'Trustless Audits without Revealing Data or Models',\n",
       "  'summary': '  There is an increasing conflict between business incentives to hide models\\nand data as trade secrets, and the societal need for algorithmic transparency.\\nFor example, a rightsholder wishing to know whether their copyrighted works\\nhave been used during training must convince the model provider to allow a\\nthird party to audit the model and data. Finding a mutually agreeable third\\nparty is difficult, and the associated costs often make this approach\\nimpractical.\\n  In this work, we show that it is possible to simultaneously allow model\\nproviders to keep their model weights (but not architecture) and data secret\\nwhile allowing other parties to trustlessly audit model and data properties. We\\ndo this by designing a protocol called ZkAudit in which model providers publish\\ncryptographic commitments of datasets and model weights, alongside a\\nzero-knowledge proof (ZKP) certifying that published commitments are derived\\nfrom training the model. Model providers can then respond to audit requests by\\nprivately computing any function F of the dataset (or model) and releasing the\\noutput of F alongside another ZKP certifying the correct execution of F. To\\nenable ZkAudit, we develop new methods of computing ZKPs for SGD on modern\\nneural nets for simple recommender systems and image classification models\\ncapable of high accuracies on ImageNet. Empirically, we show it is possible to\\nprovide trustless audits of DNNs, including copyright, censorship, and\\ncounterfactual audits with little to no loss in accuracy.\\n',\n",
       "  'published': '2024-04-06T04:43:06Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.04500v1',\n",
       "  'authors': ['Suppakit Waiwitlikhit',\n",
       "   'Ion Stoica',\n",
       "   'Yi Sun',\n",
       "   'Tatsunori Hashimoto',\n",
       "   'Daniel Kang'],\n",
       "  'h_index': 59.2,\n",
       "  'i10_index': 148.4},\n",
       " {'title': 'Do Sentence Transformers Learn Quasi-Geospatial Concepts from General\\n  Text?',\n",
       "  'summary': '  Sentence transformers are language models designed to perform semantic\\nsearch. This study investigates the capacity of sentence transformers,\\nfine-tuned on general question-answering datasets for asymmetric semantic\\nsearch, to associate descriptions of human-generated routes across Great\\nBritain with queries often used to describe hiking experiences. We find that\\nsentence transformers have some zero-shot capabilities to understand\\nquasi-geospatial concepts, such as route types and difficulty, suggesting their\\npotential utility for routing recommendation systems.\\n',\n",
       "  'published': '2024-04-05T15:22:02Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.04169v1',\n",
       "  'authors': ['Ilya Ilyankou',\n",
       "   'Aldo Lipani',\n",
       "   'Stefano Cavazzi',\n",
       "   'Xiaowei Gao',\n",
       "   'James Haworth'],\n",
       "  'h_index': 11.2,\n",
       "  'i10_index': 15.4},\n",
       " {'title': 'Learning Social Fairness Preferences from Non-Expert Stakeholder\\n  Opinions in Kidney Placement',\n",
       "  'summary': \"  Modern kidney placement incorporates several intelligent recommendation\\nsystems which exhibit social discrimination due to biases inherited from\\ntraining data. Although initial attempts were made in the literature to study\\nalgorithmic fairness in kidney placement, these methods replace true outcomes\\nwith surgeons' decisions due to the long delays involved in recording such\\noutcomes reliably. However, the replacement of true outcomes with surgeons'\\ndecisions disregards expert stakeholders' biases as well as social opinions of\\nother stakeholders who do not possess medical expertise. This paper alleviates\\nthe latter concern and designs a novel fairness feedback survey to evaluate an\\nacceptance rate predictor (ARP) that predicts a kidney's acceptance rate in a\\ngiven kidney-match pair. The survey is launched on Prolific, a crowdsourcing\\nplatform, and public opinions are collected from 85 anonymous crowd\\nparticipants. A novel social fairness preference learning algorithm is proposed\\nbased on minimizing social feedback regret computed using a novel logit-based\\nfairness feedback model. The proposed model and learning algorithm are both\\nvalidated using simulation experiments as well as Prolific data. Public\\npreferences towards group fairness notions in the context of kidney placement\\nhave been estimated and discussed in detail. The specific ARP tested in the\\nProlific survey has been deemed fair by the participants.\\n\",\n",
       "  'published': '2024-04-04T20:44:56Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.03800v1',\n",
       "  'authors': ['Mukund Telukunta',\n",
       "   'Sukruth Rao',\n",
       "   'Gabriella Stickney',\n",
       "   'Venkata Sriram Siddardh Nadendla',\n",
       "   'Casey Canfield'],\n",
       "  'h_index': 3.4,\n",
       "  'i10_index': 4.0},\n",
       " {'title': 'Understanding Language Modeling Paradigm Adaptations in Recommender\\n  Systems: Lessons Learned and Open Challenges',\n",
       "  'summary': '  The emergence of Large Language Models (LLMs) has achieved tremendous success\\nin the field of Natural Language Processing owing to diverse training paradigms\\nthat empower LLMs to effectively capture intricate linguistic patterns and\\nsemantic representations. In particular, the recent \"pre-train, prompt and\\npredict\" training paradigm has attracted significant attention as an approach\\nfor learning generalizable models with limited labeled data. In line with this\\nadvancement, these training paradigms have recently been adapted to the\\nrecommendation domain and are seen as a promising direction in both academia\\nand industry. This half-day tutorial aims to provide a thorough understanding\\nof extracting and transferring knowledge from pre-trained models learned\\nthrough different training paradigms to improve recommender systems from\\nvarious perspectives, such as generality, sparsity, effectiveness and\\ntrustworthiness. In this tutorial, we first introduce the basic concepts and a\\ngeneric architecture of the language modeling paradigm for recommendation\\npurposes. Then, we focus on recent advancements in adapting LLM-related\\ntraining strategies and optimization objectives for different recommendation\\ntasks. After that, we will systematically introduce ethical issues in LLM-based\\nrecommender systems and discuss possible approaches to assessing and mitigating\\nthem. We will also summarize the relevant datasets, evaluation metrics, and an\\nempirical study on the recommendation performance of training paradigms.\\nFinally, we will conclude the tutorial with a discussion of open challenges and\\nfuture directions.\\n',\n",
       "  'published': '2024-04-04T19:59:47Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.03788v1',\n",
       "  'authors': ['Lemei Zhang',\n",
       "   'Peng Liu',\n",
       "   'Yashar Deldjoo',\n",
       "   'Yong Zheng',\n",
       "   'Jon Atle Gulla'],\n",
       "  'h_index': 48.0,\n",
       "  'i10_index': 156.8},\n",
       " {'title': 'Sequential Recommendation for Optimizing Both Immediate Feedback and\\n  Long-term Retention',\n",
       "  'summary': \"  In the landscape of Recommender System (RS) applications, reinforcement\\nlearning (RL) has recently emerged as a powerful tool, primarily due to its\\nproficiency in optimizing long-term rewards. Nevertheless, it suffers from\\ninstability in the learning process, stemming from the intricate interactions\\namong bootstrapping, off-policy training, and function approximation. Moreover,\\nin multi-reward recommendation scenarios, designing a proper reward setting\\nthat reconciles the inner dynamics of various tasks is quite intricate. In\\nresponse to these challenges, we introduce DT4IER, an advanced decision\\ntransformer-based recommendation model that is engineered to not only elevate\\nthe effectiveness of recommendations but also to achieve a harmonious balance\\nbetween immediate user engagement and long-term retention. The DT4IER applies\\nan innovative multi-reward design that adeptly balances short and long-term\\nrewards with user-specific attributes, which serve to enhance the contextual\\nrichness of the reward sequence ensuring a more informed and personalized\\nrecommendation process. To enhance its predictive capabilities, DT4IER\\nincorporates a high-dimensional encoder, skillfully designed to identify and\\nleverage the intricate interrelations across diverse tasks. Furthermore, we\\nintegrate a contrastive learning approach within the action embedding\\npredictions, a strategy that significantly boosts the model's overall\\nperformance. Experiments on three real-world datasets demonstrate the\\neffectiveness of DT4IER against state-of-the-art Sequential Recommender Systems\\n(SRSs) and Multi-Task Learning (MTL) models in terms of both prediction\\naccuracy and effectiveness in specific tasks. The source code is accessible\\nonline to facilitate replication\\n\",\n",
       "  'published': '2024-04-04T17:55:38Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.03637v1',\n",
       "  'authors': ['Ziru Liu',\n",
       "   'Shuchang Liu',\n",
       "   'Zijian Zhang',\n",
       "   'Qingpeng Cai',\n",
       "   'Xiangyu Zhao',\n",
       "   'Kesen Zhao',\n",
       "   'Lantao Hu',\n",
       "   'Peng Jiang',\n",
       "   'Kun Gai'],\n",
       "  'h_index': 25.444444444444443,\n",
       "  'i10_index': 57.111111111111114},\n",
       " {'title': 'A Comprehensive Survey on Self-Supervised Learning for Recommendation',\n",
       "  'summary': '  Recommender systems play a crucial role in tackling the challenge of\\ninformation overload by delivering personalized recommendations based on\\nindividual user preferences. Deep learning techniques, such as RNNs, GNNs, and\\nTransformer architectures, have significantly propelled the advancement of\\nrecommender systems by enhancing their comprehension of user behaviors and\\npreferences. However, supervised learning methods encounter challenges in\\nreal-life scenarios due to data sparsity, resulting in limitations in their\\nability to learn representations effectively. To address this, self-supervised\\nlearning (SSL) techniques have emerged as a solution, leveraging inherent data\\nstructures to generate supervision signals without relying solely on labeled\\ndata. By leveraging unlabeled data and extracting meaningful representations,\\nrecommender systems utilizing SSL can make accurate predictions and\\nrecommendations even when confronted with data sparsity. In this paper, we\\nprovide a comprehensive review of self-supervised learning frameworks designed\\nfor recommender systems, encompassing a thorough analysis of over 170 papers.\\nWe conduct an exploration of nine distinct scenarios, enabling a comprehensive\\nunderstanding of SSL-enhanced recommenders in different contexts. For each\\ndomain, we elaborate on different self-supervised learning paradigms, namely\\ncontrastive learning, generative learning, and adversarial learning, so as to\\npresent technical details of how SSL enhances recommender systems in various\\ncontexts. We consistently maintain the related open-source materials at\\nhttps://github.com/HKUDS/Awesome-SSLRec-Papers.\\n',\n",
       "  'published': '2024-04-04T10:45:23Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.03354v2',\n",
       "  'authors': ['Xubin Ren', 'Wei Wei', 'Lianghao Xia', 'Chao Huang'],\n",
       "  'h_index': 61.75,\n",
       "  'i10_index': 525.25},\n",
       " {'title': 'A Directional Diffusion Graph Transformer for Recommendation',\n",
       "  'summary': \"  In real-world recommender systems, implicitly collected user feedback, while\\nabundant, often includes noisy false-positive and false-negative interactions.\\nThe possible misinterpretations of the user-item interactions pose a\\nsignificant challenge for traditional graph neural recommenders. These\\napproaches aggregate the users' or items' neighbours based on implicit\\nuser-item interactions in order to accurately capture the users' profiles. To\\naccount for and model possible noise in the users' interactions in graph neural\\nrecommenders, we propose a novel Diffusion Graph Transformer (DiffGT) model for\\ntop-k recommendation. Our DiffGT model employs a diffusion process, which\\nincludes a forward phase for gradually introducing noise to implicit\\ninteractions, followed by a reverse process to iteratively refine the\\nrepresentations of the users' hidden preferences (i.e., a denoising process).\\nIn our proposed approach, given the inherent anisotropic structure observed in\\nthe user-item interaction graph, we specifically use anisotropic and\\ndirectional Gaussian noises in the forward diffusion process. Our approach\\ndiffers from the sole use of isotropic Gaussian noises in existing diffusion\\nmodels. In the reverse diffusion process, to reverse the effect of noise added\\nearlier and recover the true users' preferences, we integrate a graph\\ntransformer architecture with a linear attention module to denoise the noisy\\nuser/item embeddings in an effective and efficient manner. In addition, such a\\nreverse diffusion process is further guided by personalised information (e.g.,\\ninteracted items) to enable the accurate estimation of the users' preferences\\non items. Our extensive experiments conclusively demonstrate the superiority of\\nour proposed graph diffusion model over ten existing state-of-the-art\\napproaches across three benchmark datasets.\\n\",\n",
       "  'published': '2024-04-04T09:52:45Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.03326v1',\n",
       "  'authors': ['Zixuan Yi', 'Xi Wang', 'Iadh Ounis'],\n",
       "  'h_index': 56.333333333333336,\n",
       "  'i10_index': 455.3333333333333},\n",
       " {'title': 'Concept -- An Evaluation Protocol on Conversation Recommender Systems\\n  with System-centric and User-centric Factors',\n",
       "  'summary': '  The conversational recommendation system (CRS) has been criticized regarding\\nits user experience in real-world scenarios, despite recent significant\\nprogress achieved in academia. Existing evaluation protocols for CRS may\\nprioritize system-centric factors such as effectiveness and fluency in\\nconversation while neglecting user-centric aspects. Thus, we propose a new and\\ninclusive evaluation protocol, Concept, which integrates both system- and\\nuser-centric factors. We conceptualise three key characteristics in\\nrepresenting such factors and further divide them into six primary abilities.\\nTo implement Concept, we adopt a LLM-based user simulator and evaluator with\\nscoring rubrics that are tailored for each primary ability. Our protocol,\\nConcept, serves a dual purpose. First, it provides an overview of the pros and\\ncons in current CRS models. Second, it pinpoints the problem of low usability\\nin the \"omnipotent\" ChatGPT and offers a comprehensive reference guide for\\nevaluating CRS, thereby setting the foundation for CRS improvement.\\n',\n",
       "  'published': '2024-04-04T08:56:48Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.03304v2',\n",
       "  'authors': ['Chen Huang',\n",
       "   'Peixin Qin',\n",
       "   'Yang Deng',\n",
       "   'Wenqiang Lei',\n",
       "   'Jiancheng Lv',\n",
       "   'Tat-Seng Chua'],\n",
       "  'h_index': 44.0,\n",
       "  'i10_index': 152.33333333333334},\n",
       " {'title': 'Does Knowledge Graph Really Matter for Recommender Systems?',\n",
       "  'summary': '  Recommender systems (RSs) are designed to provide personalized\\nrecommendations to users. Recently, knowledge graphs (KGs) have been widely\\nintroduced in RSs to improve recommendation accuracy. In this study, however,\\nwe demonstrate that RSs do not necessarily perform worse even if the KG is\\ndowngraded to the user-item interaction graph only (or removed). We propose an\\nevaluation framework KG4RecEval to systematically evaluate how much a KG\\ncontributes to the recommendation accuracy of a KG-based RS, using our defined\\nmetric KGER (KG utilization efficiency in recommendation). We consider the\\nscenarios where knowledge in a KG gets completely removed, randomly distorted\\nand decreased, and also where recommendations are for cold-start users. Our\\nextensive experiments on four commonly used datasets and a number of\\nstate-of-the-art KG-based RSs reveal that: to remove, randomly distort or\\ndecrease knowledge does not necessarily decrease recommendation accuracy, even\\nfor cold-start users. These findings inspire us to rethink how to better\\nutilize knowledge from existing KGs, whereby we discuss and provide insights\\ninto what characteristics of datasets and KG-based RSs may help improve KG\\nutilization efficiency.\\n',\n",
       "  'published': '2024-04-04T02:32:58Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.03164v1',\n",
       "  'authors': ['Haonan Zhang',\n",
       "   'Dongxia Wang',\n",
       "   'Zhu Sun',\n",
       "   'Yanhui Li',\n",
       "   'Youcheng Sun',\n",
       "   'Huizhi Liang',\n",
       "   'Wenhai Wang'],\n",
       "  'h_index': 23.714285714285715,\n",
       "  'i10_index': 38.857142857142854},\n",
       " {'title': 'A Computational Analysis of Lyric Similarity Perception',\n",
       "  'summary': \"  In musical compositions that include vocals, lyrics significantly contribute\\nto artistic expression. Consequently, previous studies have introduced the\\nconcept of a recommendation system that suggests lyrics similar to a user's\\nfavorites or personalized preferences, aiding in the discovery of lyrics among\\nmillions of tracks. However, many of these systems do not fully consider human\\nperceptions of lyric similarity, primarily due to limited research in this\\narea. To bridge this gap, we conducted a comparative analysis of computational\\nmethods for modeling lyric similarity with human perception. Results indicated\\nthat computational models based on similarities between embeddings from\\npre-trained BERT-based models, the audio from which the lyrics are derived, and\\nphonetic components are indicative of perceptual lyric similarity. This finding\\nunderscores the importance of semantic, stylistic, and phonetic similarities in\\nhuman perception about lyric similarity. We anticipate that our findings will\\nenhance the development of similarity-based lyric recommendation systems by\\noffering pseudo-labels for neural network development and introducing objective\\nevaluation metrics.\\n\",\n",
       "  'published': '2024-04-02T22:31:38Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.02342v1',\n",
       "  'authors': ['Haven Kim', 'Taketo Akama'],\n",
       "  'h_index': 0.0,\n",
       "  'i10_index': 0.0},\n",
       " {'title': 'IISAN: Efficiently Adapting Multimodal Representation for Sequential\\n  Recommendation with Decoupled PEFT',\n",
       "  'summary': '  Multimodal foundation models are transformative in sequential recommender\\nsystems, leveraging powerful representation learning capabilities. While\\nParameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation\\nmodels for recommendation tasks, most research prioritizes parameter\\nefficiency, often overlooking critical factors like GPU memory efficiency and\\ntraining speed. Addressing this gap, our paper introduces IISAN (Intra- and\\nInter-modal Side Adapted Network for Multimodal Representation), a simple\\nplug-and-play architecture using a Decoupled PEFT structure and exploiting both\\nintra- and inter-modal adaptation.\\n  IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art\\nPEFT. More importantly, it significantly reduces GPU memory usage - from 47GB\\nto just 3GB for multimodal sequential recommendation tasks. Additionally, it\\naccelerates training time per epoch from 443s to 22s compared to FFT. This is\\nalso a notable improvement over the Adapter and LoRA, which require 37-39 GB\\nGPU memory and 350-380 seconds per epoch for training.\\n  Furthermore, we propose a new composite efficiency metric, TPME\\n(Training-time, Parameter, and GPU Memory Efficiency) to alleviate the\\nprevalent misconception that \"parameter efficiency represents overall\\nefficiency\". TPME provides more comprehensive insights into practical\\nefficiency comparisons between different methods. Besides, we give an\\naccessible efficiency analysis of all PEFT and FFT approaches, which\\ndemonstrate the superiority of IISAN. We release our codes and other materials\\nat https://github.com/jjGenAILab/IISAN.\\n',\n",
       "  'published': '2024-04-02T15:58:36Z',\n",
       "  'link': 'http://arxiv.org/abs/2404.02059v1',\n",
       "  'authors': ['Junchen Fu',\n",
       "   'Xuri Ge',\n",
       "   'Xin Xin',\n",
       "   'Alexandros Karatzoglou',\n",
       "   'Ioannis Arapakis',\n",
       "   'Jie Wang',\n",
       "   'Joemon M Jose'],\n",
       "  'h_index': 45.142857142857146,\n",
       "  'i10_index': 142.42857142857142}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица успешно создана.\n",
      "Соединение с базой данных закрыто.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Параметры подключения к вашей базе данных\n",
    "conn_params = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"mysecretpassword\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Подключение к базе данных\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Создание таблицы articles\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS articles (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            title TEXT NOT NULL,\n",
    "            summary TEXT,\n",
    "            published TIMESTAMP WITH TIME ZONE,\n",
    "            link TEXT NOT NULL UNIQUE,\n",
    "            h_index FLOAT,\n",
    "            i10_index FLOAT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # Создание таблицы authors\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS authors (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            name TEXT NOT NULL,\n",
    "            article_id INTEGER,\n",
    "            FOREIGN KEY (article_id) REFERENCES articles(id),\n",
    "            UNIQUE (name, article_id)\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # Сохранение изменений\n",
    "    conn.commit()\n",
    "    print(\"Таблица успешно создана.\")\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(f\"Ошибка при создании таблицы: {error}\")\n",
    "finally:\n",
    "    # Закрытие соединения с базой данных\n",
    "    if conn is not None:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"Соединение с базой данных закрыто.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_params = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"mysecretpassword\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles2db(conn_params, data):\n",
    "    if isinstance(data, dict):\n",
    "        data = [data]\n",
    "\n",
    "    conn = None\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        for item in data:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO articles (title, summary, published, link, h_index, i10_index)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (link) DO NOTHING\n",
    "                RETURNING id;\n",
    "            \"\"\", (item[\"title\"], item[\"summary\"], item[\"published\"], item[\"link\"], item[\"h_index\"], item[\"i10_index\"]))\n",
    "            article_id = cursor.fetchone()[0] if cursor.rowcount > 0 else None\n",
    "\n",
    "            if article_id:\n",
    "                for author in item[\"authors\"]:\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO authors (name, article_id) VALUES (%s, %s)\n",
    "                        ON CONFLICT (name, article_id) DO NOTHING;\n",
    "                    \"\"\", (author, article_id)\n",
    "                    )\n",
    "\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(f\"Ошибка при создании таблицы: {error}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"Соединение с базой данных закрыто.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соединение с базой данных закрыто.\n"
     ]
    }
   ],
   "source": [
    "articles2db(conn_params, papers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles\n",
      "authors\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Установление соединения\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Выполнение запроса\n",
    "cursor.execute(\"\"\"\n",
    "SELECT table_name\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = 'public'\n",
    "ORDER BY table_name;\n",
    "\"\"\")\n",
    "\n",
    "# Получение и вывод результатов\n",
    "tables = cursor.fetchall()\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Закрытие соединения\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк из таблицы articles:\n",
      "(1, 'Can Large Language Models Assess Serendipity in Recommender Systems?', \"  Serendipity-oriented recommender systems aim to counteract\\nover-specialization in user preferences. However, evaluating a user's\\nserendipitous response towards a recommended item can be challenging because of\\nits emotional nature. In this study, we address this issue by leveraging the\\nrich knowledge of large language models (LLMs), which can perform a variety of\\ntasks. First, this study explored the alignment between serendipitous\\nevaluations made by LLMs and those made by humans. In this investigation, a\\nbinary classification task was given to the LLMs to predict whether a user\\nwould find the recommended item serendipitously. The predictive performances of\\nthree LLMs on a benchmark dataset in which humans assigned the ground truth of\\nserendipitous items were measured. The experimental findings reveal that\\nLLM-based assessment methods did not have a very high agreement rate with human\\nassessments. However, they performed as well as or better than the baseline\\nmethods. Further validation results indicate that the number of user rating\\nhistories provided to LLM prompts should be carefully chosen to avoid both\\ninsufficient and excessive inputs and that the output of LLMs that show high\\nclassification performance is difficult to interpret.\\n\", datetime.datetime(2024, 4, 11, 6, 22, 56, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.07499v1', 2.0, 0.0)\n",
      "(2, 'NFARec: A Negative Feedback-Aware Recommender Model', \"  Graph neural network (GNN)-based models have been extensively studied for\\nrecommendations, as they can extract high-order collaborative signals\\naccurately which is required for high-quality recommender systems. However,\\nthey neglect the valuable information gained through negative feedback in two\\naspects: (1) different users might hold opposite feedback on the same item,\\nwhich hampers optimal information propagation in GNNs, and (2) even when an\\nitem vastly deviates from users' preferences, they might still choose it and\\nprovide a negative rating. In this paper, we propose a negative feedback-aware\\nrecommender model (NFARec) that maximizes the leverage of negative feedback. To\\ntransfer information to multi-hop neighbors along an optimal path effectively,\\nNFARec adopts a feedback-aware correlation that guides hypergraph convolutions\\n(HGCs) to learn users' structural representations. Moreover, NFARec\\nincorporates an auxiliary task - predicting the feedback sentiment polarity\\n(i.e., positive or negative) of the next interaction - based on the Transformer\\nHawkes Process. The task is beneficial for understanding users by learning the\\nsentiment expressed in their previous sequential feedback patterns and\\npredicting future interactions. Extensive experiments demonstrate that NFARec\\noutperforms competitive baselines. Our source code and data are released at\\nhttps://github.com/WangXFng/NFARec.\\n\", datetime.datetime(2024, 4, 10, 10, 45, 30, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.06900v1', 31.8, 69.0)\n",
      "(3, \"Echo Chambers in the Age of Algorithms: An Audit of Twitter's Friend\\n  Recommender System\", \"  The presence of political misinformation and ideological echo chambers on\\nsocial media platforms is concerning given the important role that these sites\\nplay in the public's exposure to news and current events. Algorithmic systems\\nemployed on these platforms are presumed to play a role in these phenomena, but\\nlittle is known about their mechanisms and effects. In this work, we conduct an\\nalgorithmic audit of Twitter's Who-To-Follow friend recommendation system, the\\nfirst empirical audit that investigates the impact of this algorithm in-situ.\\nWe create automated Twitter accounts that initially follow left and right\\naffiliated U.S. politicians during the 2022 U.S. midterm elections and then\\ngrow their information networks using the platform's recommender system. We\\npair the experiment with an observational study of Twitter users who already\\nfollow the same politicians. Broadly, we find that while following the\\nrecommendation algorithm leads accounts into dense and reciprocal neighborhoods\\nthat structurally resemble echo chambers, the recommender also results in less\\npolitical homogeneity of a user's network compared to accounts growing their\\nnetworks through social endorsement. Furthermore, accounts that exclusively\\nfollowed users recommended by the algorithm had fewer opportunities to\\nencounter content centered on false or misleading election narratives compared\\nto choosing friends based on social endorsement.\\n\", datetime.datetime(2024, 4, 9, 16, 12, 22, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.06422v1', 26.0, 45.5)\n",
      "(4, 'DRE: Generating Recommendation Explanations by Aligning Large Language\\n  Models at Data-level', '  Recommendation systems play a crucial role in various domains, suggesting\\nitems based on user behavior.However, the lack of transparency in presenting\\nrecommendations can lead to user confusion. In this paper, we introduce\\nData-level Recommendation Explanation (DRE), a non-intrusive explanation\\nframework for black-box recommendation models.Different from existing methods,\\nDRE does not require any intermediary representations of the recommendation\\nmodel or latent alignment training, mitigating potential performance issues.We\\npropose a data-level alignment method, leveraging large language models to\\nreason relationships between user data and recommended items.Additionally, we\\naddress the challenge of enriching the details of the explanation by\\nintroducing target-aware user preference distillation, utilizing item reviews.\\nExperimental results on benchmark datasets demonstrate the effectiveness of the\\nDRE in providing accurate and user-centric explanations, enhancing user\\nengagement with recommended item.\\n', datetime.datetime(2024, 4, 9, 13, 44, 16, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.06311v1', 19.666666666666668, 45.333333333333336)\n",
      "(5, 'Dimensionality Reduction in Sentence Transformer Vector Databases with\\n  Fast Fourier Transform', '  Dimensionality reduction in vector databases is pivotal for streamlining AI\\ndata management, enabling efficient storage, faster computation, and improved\\nmodel performance. This paper explores the benefits of reducing vector database\\ndimensions, with a focus on computational efficiency and overcoming the curse\\nof dimensionality. We introduce a novel application of Fast Fourier Transform\\n(FFT) to dimensionality reduction, a method previously underexploited in this\\ncontext. By demonstrating its utility across various AI domains, including\\nRetrieval-Augmented Generation (RAG) models and image processing, this\\nFFT-based approach promises to improve data retrieval processes and enhance the\\nefficiency and scalability of AI solutions. The incorporation of FFT may not\\nonly optimize operations in real-time processing and recommendation systems but\\nalso extend to advanced image processing techniques, where dimensionality\\nreduction can significantly improve performance and analysis efficiency. This\\npaper advocates for the broader adoption of FFT in vector database management,\\nmarking a significant stride towards addressing the challenges of data volume\\nand complexity in AI research and applications. Unlike many existing\\napproaches, we directly handle the embedding vectors produced by the model\\nafter processing a test input.\\n', datetime.datetime(2024, 4, 9, 13, 2, 22, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.06278v1', 0.0, 0.0)\n",
      "(6, 'Exploring Diverse Sounds: Identifying Outliers in a Music Corpus', \"  Existing research on music recommendation systems primarily focuses on\\nrecommending similar music, thereby often neglecting diverse and distinctive\\nmusical recordings. Musical outliers can provide valuable insights due to the\\ninherent diversity of music itself. In this paper, we explore music outliers,\\ninvestigating their potential usefulness for music discovery and recommendation\\nsystems. We argue that not all outliers should be treated as noise, as they can\\noffer interesting perspectives and contribute to a richer understanding of an\\nartist's work. We introduce the concept of 'Genuine' music outliers and provide\\na definition for them. These genuine outliers can reveal unique aspects of an\\nartist's repertoire and hold the potential to enhance music discovery by\\nexposing listeners to novel and diverse musical experiences.\\n\", datetime.datetime(2024, 4, 9, 8, 10, 4, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.06103v1', 24.75, 40.75)\n",
      "(7, 'End-to-end training of Multimodal Model and ranking Model', '  Traditional recommender systems heavily rely on ID features, which often\\nencounter challenges related to cold-start and generalization. Modeling\\npre-extracted content features can mitigate these issues, but is still a\\nsuboptimal solution due to the discrepancies between training tasks and model\\nparameters. End-to-end training presents a promising solution for these\\nproblems, yet most of the existing works mainly focus on retrieval models,\\nleaving the multimodal techniques under-utilized. In this paper, we propose an\\nindustrial multimodal recommendation framework named EM3: End-to-end training\\nof Multimodal Model and ranking Model, which sufficiently utilizes multimodal\\ninformation and allows personalized ranking tasks to directly train the core\\nmodules in the multimodal model to obtain more task-oriented content features,\\nwithout overburdening resource consumption. First, we propose Fusion-Q-Former,\\nwhich consists of transformers and a set of trainable queries, to fuse\\ndifferent modalities and generate fixed-length and robust multimodal\\nembeddings. Second, in our sequential modeling for user content interest, we\\nutilize Low-Rank Adaptation technique to alleviate the conflict between huge\\nresource consumption and long sequence length. Third, we propose a novel\\nContent-ID-Contrastive learning task to complement the advantages of content\\nand ID by aligning them with each other, obtaining more task-oriented content\\nembeddings and more generalized ID embeddings. In experiments, we implement EM3\\non different ranking models in two scenario, achieving significant improvements\\nin both offline evaluation and online A/B test, verifying the generalizability\\nof our method. Ablation studies and visualization are also performed.\\nFurthermore, we also conduct experiments on two public datasets to show that\\nour proposed method outperforms the state-of-the-art methods.\\n', datetime.datetime(2024, 4, 9, 7, 32, 55, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.06078v1', 37.92307692307692, 180.15384615384616)\n",
      "(8, 'Wasserstein Dependent Graph Attention Network for Collaborative\\n  Filtering with Uncertainty', '  Collaborative filtering (CF) is an essential technique in recommender systems\\nthat provides personalized recommendations by only leveraging user-item\\ninteractions. However, most CF methods represent users and items as fixed\\npoints in the latent space, lacking the ability to capture uncertainty. In this\\npaper, we propose a novel approach, called the Wasserstein dependent Graph\\nATtention network (W-GAT), for collaborative filtering with uncertainty. We\\nutilize graph attention network and Wasserstein distance to address the\\nlimitations of LightGCN and Kullback-Leibler divergence (KL) divergence to\\nlearn Gaussian embedding for each user and item. Additionally, our method\\nincorporates Wasserstein-dependent mutual information further to increase the\\nsimilarity between positive pairs and to tackle the challenges induced by KL\\ndivergence. Experimental results on three benchmark datasets show the\\nsuperiority of W-GAT compared to several representative baselines. Extensive\\nexperimental analysis validates the effectiveness of W-GAT in capturing\\nuncertainty by modeling the range of user preferences and categories associated\\nwith items.\\n', datetime.datetime(2024, 4, 9, 2, 51, 9, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.05962v1', 55.6, 142.0)\n",
      "(9, 'Balancing Information Perception with Yin-Yang: Agent-Based Information\\n  Neutrality Model for Recommendation Systems', \"  While preference-based recommendation algorithms effectively enhance user\\nengagement by recommending personalized content, they often result in the\\ncreation of ``filter bubbles''. These bubbles restrict the range of information\\nusers interact with, inadvertently reinforcing their existing viewpoints.\\nPrevious research has focused on modifying these underlying algorithms to\\ntackle this issue. Yet, approaches that maintain the integrity of the original\\nalgorithms remain largely unexplored. This paper introduces an Agent-based\\nInformation Neutrality model grounded in the Yin-Yang theory, namely, AbIN.\\nThis innovative approach targets the imbalance in information perception within\\nexisting recommendation systems. It is designed to integrate with these\\npreference-based systems, ensuring the delivery of recommendations with neutral\\ninformation. Our empirical evaluation of this model proved its efficacy,\\nshowcasing its capacity to expand information diversity while respecting user\\npreferences. Consequently, AbIN emerges as an instrumental tool in mitigating\\nthe negative impact of filter bubbles on information consumption.\\n\", datetime.datetime(2024, 4, 7, 10, 16, 22, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.04906v1', 21.833333333333332, 75.5)\n",
      "(10, 'Trustless Audits without Revealing Data or Models', '  There is an increasing conflict between business incentives to hide models\\nand data as trade secrets, and the societal need for algorithmic transparency.\\nFor example, a rightsholder wishing to know whether their copyrighted works\\nhave been used during training must convince the model provider to allow a\\nthird party to audit the model and data. Finding a mutually agreeable third\\nparty is difficult, and the associated costs often make this approach\\nimpractical.\\n  In this work, we show that it is possible to simultaneously allow model\\nproviders to keep their model weights (but not architecture) and data secret\\nwhile allowing other parties to trustlessly audit model and data properties. We\\ndo this by designing a protocol called ZkAudit in which model providers publish\\ncryptographic commitments of datasets and model weights, alongside a\\nzero-knowledge proof (ZKP) certifying that published commitments are derived\\nfrom training the model. Model providers can then respond to audit requests by\\nprivately computing any function F of the dataset (or model) and releasing the\\noutput of F alongside another ZKP certifying the correct execution of F. To\\nenable ZkAudit, we develop new methods of computing ZKPs for SGD on modern\\nneural nets for simple recommender systems and image classification models\\ncapable of high accuracies on ImageNet. Empirically, we show it is possible to\\nprovide trustless audits of DNNs, including copyright, censorship, and\\ncounterfactual audits with little to no loss in accuracy.\\n', datetime.datetime(2024, 4, 6, 4, 43, 6, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.04500v1', 59.2, 148.4)\n",
      "(11, 'Do Sentence Transformers Learn Quasi-Geospatial Concepts from General\\n  Text?', '  Sentence transformers are language models designed to perform semantic\\nsearch. This study investigates the capacity of sentence transformers,\\nfine-tuned on general question-answering datasets for asymmetric semantic\\nsearch, to associate descriptions of human-generated routes across Great\\nBritain with queries often used to describe hiking experiences. We find that\\nsentence transformers have some zero-shot capabilities to understand\\nquasi-geospatial concepts, such as route types and difficulty, suggesting their\\npotential utility for routing recommendation systems.\\n', datetime.datetime(2024, 4, 5, 15, 22, 2, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.04169v1', 11.2, 15.4)\n",
      "(12, 'Learning Social Fairness Preferences from Non-Expert Stakeholder\\n  Opinions in Kidney Placement', \"  Modern kidney placement incorporates several intelligent recommendation\\nsystems which exhibit social discrimination due to biases inherited from\\ntraining data. Although initial attempts were made in the literature to study\\nalgorithmic fairness in kidney placement, these methods replace true outcomes\\nwith surgeons' decisions due to the long delays involved in recording such\\noutcomes reliably. However, the replacement of true outcomes with surgeons'\\ndecisions disregards expert stakeholders' biases as well as social opinions of\\nother stakeholders who do not possess medical expertise. This paper alleviates\\nthe latter concern and designs a novel fairness feedback survey to evaluate an\\nacceptance rate predictor (ARP) that predicts a kidney's acceptance rate in a\\ngiven kidney-match pair. The survey is launched on Prolific, a crowdsourcing\\nplatform, and public opinions are collected from 85 anonymous crowd\\nparticipants. A novel social fairness preference learning algorithm is proposed\\nbased on minimizing social feedback regret computed using a novel logit-based\\nfairness feedback model. The proposed model and learning algorithm are both\\nvalidated using simulation experiments as well as Prolific data. Public\\npreferences towards group fairness notions in the context of kidney placement\\nhave been estimated and discussed in detail. The specific ARP tested in the\\nProlific survey has been deemed fair by the participants.\\n\", datetime.datetime(2024, 4, 4, 20, 44, 56, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.03800v1', 3.4, 4.0)\n",
      "(13, 'Understanding Language Modeling Paradigm Adaptations in Recommender\\n  Systems: Lessons Learned and Open Challenges', '  The emergence of Large Language Models (LLMs) has achieved tremendous success\\nin the field of Natural Language Processing owing to diverse training paradigms\\nthat empower LLMs to effectively capture intricate linguistic patterns and\\nsemantic representations. In particular, the recent \"pre-train, prompt and\\npredict\" training paradigm has attracted significant attention as an approach\\nfor learning generalizable models with limited labeled data. In line with this\\nadvancement, these training paradigms have recently been adapted to the\\nrecommendation domain and are seen as a promising direction in both academia\\nand industry. This half-day tutorial aims to provide a thorough understanding\\nof extracting and transferring knowledge from pre-trained models learned\\nthrough different training paradigms to improve recommender systems from\\nvarious perspectives, such as generality, sparsity, effectiveness and\\ntrustworthiness. In this tutorial, we first introduce the basic concepts and a\\ngeneric architecture of the language modeling paradigm for recommendation\\npurposes. Then, we focus on recent advancements in adapting LLM-related\\ntraining strategies and optimization objectives for different recommendation\\ntasks. After that, we will systematically introduce ethical issues in LLM-based\\nrecommender systems and discuss possible approaches to assessing and mitigating\\nthem. We will also summarize the relevant datasets, evaluation metrics, and an\\nempirical study on the recommendation performance of training paradigms.\\nFinally, we will conclude the tutorial with a discussion of open challenges and\\nfuture directions.\\n', datetime.datetime(2024, 4, 4, 19, 59, 47, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.03788v1', 48.0, 156.8)\n",
      "(14, 'Sequential Recommendation for Optimizing Both Immediate Feedback and\\n  Long-term Retention', \"  In the landscape of Recommender System (RS) applications, reinforcement\\nlearning (RL) has recently emerged as a powerful tool, primarily due to its\\nproficiency in optimizing long-term rewards. Nevertheless, it suffers from\\ninstability in the learning process, stemming from the intricate interactions\\namong bootstrapping, off-policy training, and function approximation. Moreover,\\nin multi-reward recommendation scenarios, designing a proper reward setting\\nthat reconciles the inner dynamics of various tasks is quite intricate. In\\nresponse to these challenges, we introduce DT4IER, an advanced decision\\ntransformer-based recommendation model that is engineered to not only elevate\\nthe effectiveness of recommendations but also to achieve a harmonious balance\\nbetween immediate user engagement and long-term retention. The DT4IER applies\\nan innovative multi-reward design that adeptly balances short and long-term\\nrewards with user-specific attributes, which serve to enhance the contextual\\nrichness of the reward sequence ensuring a more informed and personalized\\nrecommendation process. To enhance its predictive capabilities, DT4IER\\nincorporates a high-dimensional encoder, skillfully designed to identify and\\nleverage the intricate interrelations across diverse tasks. Furthermore, we\\nintegrate a contrastive learning approach within the action embedding\\npredictions, a strategy that significantly boosts the model's overall\\nperformance. Experiments on three real-world datasets demonstrate the\\neffectiveness of DT4IER against state-of-the-art Sequential Recommender Systems\\n(SRSs) and Multi-Task Learning (MTL) models in terms of both prediction\\naccuracy and effectiveness in specific tasks. The source code is accessible\\nonline to facilitate replication\\n\", datetime.datetime(2024, 4, 4, 17, 55, 38, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.03637v1', 25.444444444444443, 57.111111111111114)\n",
      "(15, 'A Comprehensive Survey on Self-Supervised Learning for Recommendation', '  Recommender systems play a crucial role in tackling the challenge of\\ninformation overload by delivering personalized recommendations based on\\nindividual user preferences. Deep learning techniques, such as RNNs, GNNs, and\\nTransformer architectures, have significantly propelled the advancement of\\nrecommender systems by enhancing their comprehension of user behaviors and\\npreferences. However, supervised learning methods encounter challenges in\\nreal-life scenarios due to data sparsity, resulting in limitations in their\\nability to learn representations effectively. To address this, self-supervised\\nlearning (SSL) techniques have emerged as a solution, leveraging inherent data\\nstructures to generate supervision signals without relying solely on labeled\\ndata. By leveraging unlabeled data and extracting meaningful representations,\\nrecommender systems utilizing SSL can make accurate predictions and\\nrecommendations even when confronted with data sparsity. In this paper, we\\nprovide a comprehensive review of self-supervised learning frameworks designed\\nfor recommender systems, encompassing a thorough analysis of over 170 papers.\\nWe conduct an exploration of nine distinct scenarios, enabling a comprehensive\\nunderstanding of SSL-enhanced recommenders in different contexts. For each\\ndomain, we elaborate on different self-supervised learning paradigms, namely\\ncontrastive learning, generative learning, and adversarial learning, so as to\\npresent technical details of how SSL enhances recommender systems in various\\ncontexts. We consistently maintain the related open-source materials at\\nhttps://github.com/HKUDS/Awesome-SSLRec-Papers.\\n', datetime.datetime(2024, 4, 4, 10, 45, 23, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.03354v2', 61.75, 525.25)\n",
      "(16, 'A Directional Diffusion Graph Transformer for Recommendation', \"  In real-world recommender systems, implicitly collected user feedback, while\\nabundant, often includes noisy false-positive and false-negative interactions.\\nThe possible misinterpretations of the user-item interactions pose a\\nsignificant challenge for traditional graph neural recommenders. These\\napproaches aggregate the users' or items' neighbours based on implicit\\nuser-item interactions in order to accurately capture the users' profiles. To\\naccount for and model possible noise in the users' interactions in graph neural\\nrecommenders, we propose a novel Diffusion Graph Transformer (DiffGT) model for\\ntop-k recommendation. Our DiffGT model employs a diffusion process, which\\nincludes a forward phase for gradually introducing noise to implicit\\ninteractions, followed by a reverse process to iteratively refine the\\nrepresentations of the users' hidden preferences (i.e., a denoising process).\\nIn our proposed approach, given the inherent anisotropic structure observed in\\nthe user-item interaction graph, we specifically use anisotropic and\\ndirectional Gaussian noises in the forward diffusion process. Our approach\\ndiffers from the sole use of isotropic Gaussian noises in existing diffusion\\nmodels. In the reverse diffusion process, to reverse the effect of noise added\\nearlier and recover the true users' preferences, we integrate a graph\\ntransformer architecture with a linear attention module to denoise the noisy\\nuser/item embeddings in an effective and efficient manner. In addition, such a\\nreverse diffusion process is further guided by personalised information (e.g.,\\ninteracted items) to enable the accurate estimation of the users' preferences\\non items. Our extensive experiments conclusively demonstrate the superiority of\\nour proposed graph diffusion model over ten existing state-of-the-art\\napproaches across three benchmark datasets.\\n\", datetime.datetime(2024, 4, 4, 9, 52, 45, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.03326v1', 56.333333333333336, 455.3333333333333)\n",
      "(17, 'Concept -- An Evaluation Protocol on Conversation Recommender Systems\\n  with System-centric and User-centric Factors', '  The conversational recommendation system (CRS) has been criticized regarding\\nits user experience in real-world scenarios, despite recent significant\\nprogress achieved in academia. Existing evaluation protocols for CRS may\\nprioritize system-centric factors such as effectiveness and fluency in\\nconversation while neglecting user-centric aspects. Thus, we propose a new and\\ninclusive evaluation protocol, Concept, which integrates both system- and\\nuser-centric factors. We conceptualise three key characteristics in\\nrepresenting such factors and further divide them into six primary abilities.\\nTo implement Concept, we adopt a LLM-based user simulator and evaluator with\\nscoring rubrics that are tailored for each primary ability. Our protocol,\\nConcept, serves a dual purpose. First, it provides an overview of the pros and\\ncons in current CRS models. Second, it pinpoints the problem of low usability\\nin the \"omnipotent\" ChatGPT and offers a comprehensive reference guide for\\nevaluating CRS, thereby setting the foundation for CRS improvement.\\n', datetime.datetime(2024, 4, 4, 8, 56, 48, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.03304v2', 44.0, 152.33333333333334)\n",
      "(18, 'Does Knowledge Graph Really Matter for Recommender Systems?', '  Recommender systems (RSs) are designed to provide personalized\\nrecommendations to users. Recently, knowledge graphs (KGs) have been widely\\nintroduced in RSs to improve recommendation accuracy. In this study, however,\\nwe demonstrate that RSs do not necessarily perform worse even if the KG is\\ndowngraded to the user-item interaction graph only (or removed). We propose an\\nevaluation framework KG4RecEval to systematically evaluate how much a KG\\ncontributes to the recommendation accuracy of a KG-based RS, using our defined\\nmetric KGER (KG utilization efficiency in recommendation). We consider the\\nscenarios where knowledge in a KG gets completely removed, randomly distorted\\nand decreased, and also where recommendations are for cold-start users. Our\\nextensive experiments on four commonly used datasets and a number of\\nstate-of-the-art KG-based RSs reveal that: to remove, randomly distort or\\ndecrease knowledge does not necessarily decrease recommendation accuracy, even\\nfor cold-start users. These findings inspire us to rethink how to better\\nutilize knowledge from existing KGs, whereby we discuss and provide insights\\ninto what characteristics of datasets and KG-based RSs may help improve KG\\nutilization efficiency.\\n', datetime.datetime(2024, 4, 4, 2, 32, 58, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.03164v1', 23.714285714285715, 38.857142857142854)\n",
      "(19, 'A Computational Analysis of Lyric Similarity Perception', \"  In musical compositions that include vocals, lyrics significantly contribute\\nto artistic expression. Consequently, previous studies have introduced the\\nconcept of a recommendation system that suggests lyrics similar to a user's\\nfavorites or personalized preferences, aiding in the discovery of lyrics among\\nmillions of tracks. However, many of these systems do not fully consider human\\nperceptions of lyric similarity, primarily due to limited research in this\\narea. To bridge this gap, we conducted a comparative analysis of computational\\nmethods for modeling lyric similarity with human perception. Results indicated\\nthat computational models based on similarities between embeddings from\\npre-trained BERT-based models, the audio from which the lyrics are derived, and\\nphonetic components are indicative of perceptual lyric similarity. This finding\\nunderscores the importance of semantic, stylistic, and phonetic similarities in\\nhuman perception about lyric similarity. We anticipate that our findings will\\nenhance the development of similarity-based lyric recommendation systems by\\noffering pseudo-labels for neural network development and introducing objective\\nevaluation metrics.\\n\", datetime.datetime(2024, 4, 2, 22, 31, 38, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.02342v1', 0.0, 0.0)\n",
      "(20, 'IISAN: Efficiently Adapting Multimodal Representation for Sequential\\n  Recommendation with Decoupled PEFT', '  Multimodal foundation models are transformative in sequential recommender\\nsystems, leveraging powerful representation learning capabilities. While\\nParameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation\\nmodels for recommendation tasks, most research prioritizes parameter\\nefficiency, often overlooking critical factors like GPU memory efficiency and\\ntraining speed. Addressing this gap, our paper introduces IISAN (Intra- and\\nInter-modal Side Adapted Network for Multimodal Representation), a simple\\nplug-and-play architecture using a Decoupled PEFT structure and exploiting both\\nintra- and inter-modal adaptation.\\n  IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art\\nPEFT. More importantly, it significantly reduces GPU memory usage - from 47GB\\nto just 3GB for multimodal sequential recommendation tasks. Additionally, it\\naccelerates training time per epoch from 443s to 22s compared to FFT. This is\\nalso a notable improvement over the Adapter and LoRA, which require 37-39 GB\\nGPU memory and 350-380 seconds per epoch for training.\\n  Furthermore, we propose a new composite efficiency metric, TPME\\n(Training-time, Parameter, and GPU Memory Efficiency) to alleviate the\\nprevalent misconception that \"parameter efficiency represents overall\\nefficiency\". TPME provides more comprehensive insights into practical\\nefficiency comparisons between different methods. Besides, we give an\\naccessible efficiency analysis of all PEFT and FFT approaches, which\\ndemonstrate the superiority of IISAN. We release our codes and other materials\\nat https://github.com/jjGenAILab/IISAN.\\n', datetime.datetime(2024, 4, 2, 15, 58, 36, tzinfo=datetime.timezone.utc), 'http://arxiv.org/abs/2404.02059v1', 45.142857142857146, 142.42857142857142)\n",
      "\n",
      "\n",
      "Первые 5 строк из таблицы authors:\n",
      "(1, 'Yu Tokutake', 1)\n",
      "(2, 'Kazushi Okamoto', 1)\n",
      "(3, 'Xinfeng Wang', 2)\n",
      "(4, 'Fumiyo Fukumoto', 2)\n",
      "(5, 'Jin Cui', 2)\n",
      "(6, 'Yoshimi Suzuki', 2)\n",
      "(7, 'Dongjin Yu', 2)\n",
      "(8, 'Kayla Duskin', 3)\n",
      "(9, 'Joseph S. Schafer', 3)\n",
      "(10, 'Jevin D. West', 3)\n",
      "(11, 'Emma S. Spiro', 3)\n",
      "(12, 'Shen Gao', 4)\n",
      "(13, 'Yifan Wang', 4)\n",
      "(14, 'Jiabao Fang', 4)\n",
      "(15, 'Lisi Chen', 4)\n",
      "(16, 'Peng Han', 4)\n",
      "(17, 'Shuo Shang', 4)\n",
      "(18, 'Vitaly Bulgakov', 5)\n",
      "(19, 'Alec Segal', 5)\n",
      "(20, 'Le Cai', 6)\n",
      "(21, 'Sam Ferguson', 6)\n",
      "(22, 'Gengfa Fang', 6)\n",
      "(23, 'Hani Alshamrani', 6)\n",
      "(24, 'Xiuqi Deng', 7)\n",
      "(25, 'Lu Xu', 7)\n",
      "(26, 'Xiyao Li', 7)\n",
      "(27, 'Jinkai Yu', 7)\n",
      "(28, 'Erpeng Xue', 7)\n",
      "(29, 'Zhongyuan Wang', 7)\n",
      "(30, 'Di Zhang', 7)\n",
      "(31, 'Zhaojie Liu', 7)\n",
      "(32, 'Guorui Zhou', 7)\n",
      "(33, 'Yang Song', 7)\n",
      "(34, 'Na Mou', 7)\n",
      "(35, 'Shen Jiang', 7)\n",
      "(36, 'Han Li', 7)\n",
      "(37, 'Haoxuan Li', 8)\n",
      "(38, 'Yuanxin Ouyang', 8)\n",
      "(39, 'Zhuang Liu', 8)\n",
      "(40, 'Wenge Rong', 8)\n",
      "(41, 'Zhang Xiong', 8)\n",
      "(42, 'Mengyan Wang', 9)\n",
      "(43, 'Yuxuan Hu', 9)\n",
      "(44, 'Shiqing Wu', 9)\n",
      "(45, 'Weihua Li', 9)\n",
      "(46, 'Quan Bai', 9)\n",
      "(47, 'Verica Rupar', 9)\n",
      "(48, 'Suppakit Waiwitlikhit', 10)\n",
      "(49, 'Ion Stoica', 10)\n",
      "(50, 'Yi Sun', 10)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_first_five_rows_of_each_table(conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Получение списка всех таблиц в схеме public\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'public'\n",
    "    \"\"\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    for table_name in tables:\n",
    "        print(f\"Первые 5 строк из таблицы {table_name[0]}:\")\n",
    "        try:\n",
    "            cursor.execute(f\"SELECT * FROM {table_name[0]} LIMIT 50;\")\n",
    "            rows = cursor.fetchall()\n",
    "            for row in rows:\n",
    "                print(row)\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Ошибка при выборке из таблицы {table_name[0]}: {e}\")\n",
    "        print(\"\\n\")  # Добавляем пустую строку между таблицами для лучшей читаемости\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "print_first_five_rows_of_each_table(conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaf752bc84446708fb6de80359f87d67f557da8e06f21ebc4642efce91487fd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
